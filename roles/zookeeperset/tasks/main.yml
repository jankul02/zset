- name: print configuration values    
  debug:
    msg:  '{{ app }} has  autoscale.range.min: {{ autoscale.range.min }}  '

- name: calculate replicas  
  #TODO real calculation upon node metrics and external hints
  set_fact:
    replicas: '{{autoscale.range.min | int}}'

- name: print replicas    
  debug:
    msg:  'replicas {{ replicas }}'    
  



- name: init specs    
  set_fact:
    instancesspeclist:  []  
 
- name: print allspecs   
  debug:
    msg:  'instance {{ item }}  '    
  loop: "{{ instances }}"



# - name: collect allspecs with added preset labels    
#   set_fact:
#      instancesspeclist:  "{{ instancesspeclist + [ item | combine({'spec':(item.spec  | default({}))|combine({ 'matchLabels': ( spec.matchLabels | default([]) ) + [ { 'podnamerequired': 'zk-'+ item.id } ] }  ) }) ] }}"
#   loop: "{{ instances }}"


- name: print allspecs    
  debug:
    msg:  'instancesspec {{ item }}  '    
  loop: "{{ instancesspeclist }}"

- name: print specs
  debug:
    msg:  'spec: {{ item.spec }}'    
  loop: "{{ instances }}"




- name: Search for all preset labelled app
  community.kubernetes.k8s_info:
    kind: PodPreset
    api_version: v1alpha1
    # label_selectors:
    #   - 'app = {{app}}'
  register: podpresetsbefore

- name: print podpresetsbefore
  debug:
    msg:  '{{ podpresetsbefore }}'    

- name: define pod presets
  community.kubernetes.k8s:
    definition:
      apiVersion: redhatcop.redhat.io/v1alpha1
      kind: PodPreset
      metadata:
        name: 'zk-{{ item.id }}'
        namespace: '{{namespace}}'
      spec: 
        selector:
          matchLabels:
            app: '{{app}}'
            podnamerequired: 'zk-{{ item.id }}'
        env: '{{ item.spec.env | default([]) }}'
        volumes: '{{ item.spec.volumes | default([]) }}'
        volumeMounts: '{{ item.spec.volumeMounts | default([]) }}'
  loop: "{{ instances }}"
  register: presettingresults

- name: print presettingresult
  debug:
    msg:  '{{ presettingresults }}'    




# - name: define pod presets
#   community.kubernetes.k8s:
#     definition:
#       apiVersion: redhatcop.redhat.io/v1alpha1
#       kind: PodPreset
#       metadata:
#         name: 'zk-0'
#         namespace: '{{namespace}}'
#       spec:
#         selector:
#           matchLabels:
#             app: '{{app}}'
#             podnamerequired: 'zk-0'
#         env: 
#           - name: ZOOKEEPER_TOOLS_LOG4J_LOGLEVEL
#             value: INFO
#           - name: ZOOKEEPER_LOG4J_ROOT_LOGLEVEL
#             value: INFO
#           - name: WELCOME_MESSAGE
#             value: "Hello hopla hey hey 0000"
#           - name: ADDITIONAL_PARAM
#             value: " 231122"
#         volumeMounts:
#           # name must match the volume name below
#           - name: zksecrets0
#             mountPath: /mnt/zksecrets
#           # The secret data is exposed to Containers in the Pod through a Volume.
#         volumes:
#           - name: zksecrets0
#             secret:
#               secretName: zksecrets0
#  loop: "{{ instances }}"
  


# - name: nginx services
#   community.kubernetes.k8s:
#     state: present
#     definition:
#       apiVersion: v1
#       kind: Service
#       metadata:
#         name: my-nginx{{item}}-svc
#         namespace: default        
#         labels:
#           app: nginx{{item}}
#       spec:
#         type: LoadBalancer
#         ports:
#         - port: 80
#         selector:
#           app: nginx{{item}}    
#   loop: "{{ range(0, 20, 1)|list }}"

# - name: nginx apps
#   community.kubernetes.k8s:
#     state: present
#     definition:
#       apiVersion: apps/v1
#       kind: Deployment
#       metadata:
#         name: my-nginx{{item}}
#         namespace: default            
#         labels:
#           app: nginx{{item}}
#       spec:
#         replicas: 1
#         selector:
#           matchLabels:
#             app: nginx{{item}}
#         template:
#           metadata:
#             labels:
#               app: nginx{{item}}
#           spec:
#             containers:
#             - name: nginx{{item}}
#               image: nginx:1.14.2
#               ports:
#               - containerPort: 80
#   loop: "{{ range(0, 20, 1)|list }}"



- name: get the configname (uuid) of the comming deployment    
  set_fact:
    configname: "zk-config-{{ data  | default({}) | to_uuid  }}"  


- name: append a stamp to have non empty data   
  set_fact:
    data_to_set: "{{ data  | default({}) | combine({ 'metacfg': 'CONFIGNAME='+configname}) }}"

- name: create cnf
  community.kubernetes.k8s:
    definition:
      apiVersion: v1
      kind: ConfigMap
      metadata: 
        name: zk-global
        namespace: '{{ namespace }}'
        labels:
          app: '{{ app }}'
          configmap: zk-global
      data: '{{data_to_set}}'  
    state: present


- name: Service hsvc
  community.kubernetes.k8s:
    definition:
      kind: Service
      apiVersion: v1
      metadata:
        name: zk-hs
        namespace: '{{ namespace }}'
        labels:
          app: '{{ app }}'    
      spec:
        ports:
        - port: 2888
          name: tcp-server
        - port: 3888
          name: tcp-leader-el
        clusterIP: None
        selector:
          app: '{{ app }}'   



          
- name: Service zookeepersetapp-csvc
  community.kubernetes.k8s:
    definition:
      kind: Service
      apiVersion: v1
      metadata:
        name: zk-cs
        namespace: '{{ namespace }}'
        labels:
          app: '{{ app }}'    
      spec:
        ports:
        - port: 2181
          name: client
        selector:
          app: '{{ app }}'    

- name: policy for pod disruption budget 
  community.kubernetes.k8s:
    definition:
      apiVersion: policy/v1
      kind: PodDisruptionBudget
      metadata:
        name: zk-pdb
        namespace: '{{ namespace }}'
      spec:
        selector:
          matchLabels:
            app: '{{ app }}'    
        #TODO: variable for maxUnavailable
        maxUnavailable: 1  

- name: volumemounts
  set_fact:
    volumemounts:
      - name: datadir
        mountPath: /var/lib/zookeeper
      - name: zk-global
        mountPath: /mnt/zk-global

- name: volumes
  set_fact:
    volumes:
      - name: zk-global
        configMap:
          name: zk-global       

- name:  ZookeeperSet depends on
  community.kubernetes.k8s_info:
    kind: '{{item.kind}}'
    name: '{{item.name}}'
    namespace: '{{namespace}}'
  loop: '{{ updateOnChange }}'  
  register: updateonchangeresources

- name: print updateonchangeresources
  debug:
    msg:  '{{ updateonchangeresources }}'    

- name: print resources
  debug:
    msg:  'item: {{ item }}'    
  loop: '{{ updateonchangeresources.results }}'  


- name: annotate dependent update ZookeeperSet
  set_fact:
      annotation: '{{ ( annotation | default([]) ) + item.resources }}'
  loop: '{{ updateonchangeresources.results }}'  
  when: item.resources | length > 0

- name: print dependent update ZookeeperSet
  debug:
    msg:  '{{ annotation }}'  

- name: make annotation update ZookeeperSet
  set_fact:
      rolloutAnnotation: "{{ annotation | hash('sha1') }}" 


- name: Zookeeperset deployment
  community.kubernetes.k8s:
    definition:
      apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: zk
        namespace: '{{ namespace }}'       
        labels:
          app: '{{ app }}'
      spec:
        selector:
          matchLabels:
            app: '{{ app }}'
        serviceName: zk-hs
        replicas: 3        
        updateStrategy:
          type: RollingUpdate
        podManagementPolicy: OrderedReady
        template:
          metadata:
            labels:
              app: '{{ app }}'
            annotations: 
              configname: '{{configname}}' 
              rolloutAnnotation: '{{rolloutAnnotation}}'                
          spec:
            affinity:
              podAntiAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  - labelSelector:
                      matchExpressions:
                        - key: "app"
                          operator: In
                          values:
                          - '{{ app }}'    
                    topologyKey: "kubernetes.io/hostname"
            containers:
            - name: kubernetes-zookeeper
              imagePullPolicy: Always
              image: '{{ image }}'
              resources:
                requests:
                  memory: "1Gi"
                  cpu: "0.5"
              ports:
              - containerPort: 2181
                name: tcp-client
              - containerPort: 2888
                name: tcp-server
              - containerPort: 3888
                name: tcp-leader-el            
              command:
              - sh
              - -c
              - "/etc/confluent/docker/start-zookeeper \
                 --servers={{replicas}}"
              readinessProbe:
                exec:
                  command:
                  - sh
                  - -c
                  - "/etc/confluent/docker/zookeeper-ready 2181"
                initialDelaySeconds: 10
                timeoutSeconds: 5
              livenessProbe:
                exec:
                  command:
                  - sh
                  - -c
                  - "/etc/confluent/docker/zookeeper-ready 2181"
                initialDelaySeconds: 10
                timeoutSeconds: 5
              volumeMounts: "{{ volumemounts }}"
            volumes: "{{ volumes }}"
            securityContext:
      #        runAsUser: 1000
              fsGroup: 1000
        volumeClaimTemplates:
        - metadata:
            name: datadir
          spec:
            accessModes: [ "ReadWriteOnce" ]
            resources:
              requests:
                storage: 10Gi
  register: stsresult


- name: print stsresult
  debug:
    msg:  '{{ stsresult }}'    

- name: Kafka headless service
  community.kubernetes.k8s:
    definition:
      apiVersion: v1
      kind: Service
      metadata:
        name: kafka-hs
        namespace: '{{namespace}}'
        labels:
          app: '{{kafkaapp}}'
      spec:
        ports:
          - port: 9092
            name: broker
        clusterIP: None
        selector:
          app: '{{kafkaapp}}'

- name: Kafka service
  community.kubernetes.k8s:
    definition:
      apiVersion: v1
      kind: Service
      metadata:
        name: kafka
        namespace: '{{namespace}}'
        labels:
          app: '{{kafkaapp}}'
      spec:
        ports:
          - port: 9092
            name: broker
          - port: 5556
            name: metrics
        selector:
          app: '{{kafkaapp}}'

- name: Kafka deployment
  community.kubernetes.k8s:
    definition:
      apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: kafka
        namespace: '{{namespace}}'
        labels:
          app: '{{kafkaapp}}'
      spec:
        selector:
          matchLabels:
            app: '{{kafkaapp}}'
        serviceName: kafka-hs
        podManagementPolicy: OrderedReady
        replicas: 3
        updateStrategy:
          type: RollingUpdate
        template:
          metadata:
            labels:
              app: '{{kafkaapp}}'
            annotations:
          spec:
            affinity:
              podAntiAffinity:
                preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 1
                  podAffinityTerm:
                    labelSelector:
                      matchExpressions:
                        - key: "app"
                          operator: In
                          values:
                          - '{{kafkaapp}}'
                    topologyKey: "kubernetes.io/hostname"
            containers:
            - name: cp-kafka-broker
              image: "confluentinc/cp-kafka:6.1.0"
              imagePullPolicy: "Always"
              securityContext:
                runAsUser: 0
              ports:
              - containerPort: 9092
                name: kafka
              resources:
                {}
              env:
              - name: POD_IP
                valueFrom:
                  fieldRef:
                    fieldPath: status.podIP
              - name: HOST_IP
                valueFrom:
                  fieldRef:
                    fieldPath: status.hostIP
              - name: POD_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
              - name: POD_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              - name: KAFKA_HEAP_OPTS
                value: -Xms512M -Xmx512M
              - name: KAFKA_ZOOKEEPER_CONNECT
                value: "zk-cs:2181"
              - name: KAFKA_LOG_DIRS
                value: "/opt/kafka/data-0/logs"
              - name: KAFKA_METRIC_REPORTERS
                value: "io.confluent.metrics.reporter.ConfluentMetricsReporter"
              - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS
                value: "PLAINTEXT://kafka-hs:9092"
              - name: KAFKA_LOG4J_ROOT_LOGLEVEL
                value: "DEBUG"
              - name: KAFKA_TOOLS_LOG4J_LOGLEVEL
                value: "DEBUG"
              - name: "KAFKA_LISTENER_SECURITY_PROTOCOL_MAP"
                value: "PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT"
              - name: "KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR"
                value: "3"
              # This is required because the Downward API does not yet support identification of
              # pod numbering in statefulsets. Thus, we are required to specify a command which
              # allows us to extract the pod ID for usage as the Kafka Broker ID.
              # See: https://github.com/kubernetes/kubernetes/issues/31218
              command:
              - sh
              - -exc
              - |
                echo "HOSTNAME: ${HOSTNAME} ${HOSTNAME##*-} "  && \
                export KAFKA_LOG4J_ROOT_LOGLEVEL=DEBUG && \
                export KAFKA_TOOLS_LOG4J_LOGLEVEL=DEBUG && \
                export KAFKA_BROKER_ID=${HOSTNAME##*-} && \
                export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-hs.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \
                unset KAFKA_PORT && \
                echo "KAFKA_ADVERTISED_LISTENERS="$KAFKA_ADVERTISED_LISTENERS &&
                echo "===> User" && \
                id && \
                echo "===> Configuring ..." && \
                . /etc/confluent/docker/bash-config && \
                dub ensure KAFKA_ZOOKEEPER_CONNECT  && \
                dub ensure KAFKA_ADVERTISED_LISTENERS  && \
                if [[ -z "${KAFKA_LISTENERS-}" ]]; then export KAFKA_LISTENERS &&  KAFKA_LISTENERS=$(cub listeners "$KAFKA_ADVERTISED_LISTENERS") ; fi && \
                dub path /etc/kafka/ writable  && \
                if [[ -z "${KAFKA_LOG_DIRS-}" ]]; then export KAFKA_LOG_DIRS && KAFKA_LOG_DIRS="/var/lib/kafka/data"; fi  && \
                if [[ -n "${KAFKA_ADVERTISED_PORT-}" ]]; then echo "advertised.port is deprecated. Please use KAFKA_ADVERTISED_LISTENERS instead." && exit 1 ; fi  && \
                if [[ -n "${KAFKA_ADVERTISED_HOST-}" ]]; then echo "advertised.host is deprecated. Please use KAFKA_ADVERTISED_LISTENERS instead." && exit 1 ; fi  && \
                if [[ -n "${KAFKA_HOST-}" ]];  then echo "host is deprecated. Please use KAFKA_ADVERTISED_LISTENERS instead." && exit 1 ; fi && \
                if [[ -n "${KAFKA_PORT-}" ]];  then echo ""port is deprecated. Please use KAFKA_ADVERTISED_LISTENERS instead."" && exit 1 ; fi && \
                # Set if ADVERTISED_LISTENERS has SSL:// or SASL_SSL:// endpoints. && \
                if [[ $KAFKA_ADVERTISED_LISTENERS == *"SSL://"* ]]; then echo "SSL is enabled." && \
                  dub ensure KAFKA_SSL_KEYSTORE_FILENAME && \
                  export KAFKA_SSL_KEYSTORE_LOCATION="/etc/kafka/secrets/$KAFKA_SSL_KEYSTORE_FILENAME"  && \
                  dub path "$KAFKA_SSL_KEYSTORE_LOCATION" exists  && \
                  dub ensure KAFKA_SSL_KEY_CREDENTIALS  && \
                  KAFKA_SSL_KEY_CREDENTIALS_LOCATION="/etc/kafka/secrets/$KAFKA_SSL_KEY_CREDENTIALS" && \
                  dub path "$KAFKA_SSL_KEY_CREDENTIALS_LOCATION" exists && \
                  export KAFKA_SSL_KEY_PASSWORD && \
                  KAFKA_SSL_KEY_PASSWORD=$(cat "$KAFKA_SSL_KEY_CREDENTIALS_LOCATION") && \
                  dub ensure KAFKA_SSL_KEYSTORE_CREDENTIALS && \
                  KAFKA_SSL_KEYSTORE_CREDENTIALS_LOCATION="/etc/kafka/secrets/$KAFKA_SSL_KEYSTORE_CREDENTIALS" && \
                  dub path "$KAFKA_SSL_KEYSTORE_CREDENTIALS_LOCATION" exists && \
                  export KAFKA_SSL_KEYSTORE_PASSWORD && \
                  KAFKA_SSL_KEYSTORE_PASSWORD=$(cat "$KAFKA_SSL_KEYSTORE_CREDENTIALS_LOCATION") && \
                  if [[ -n "${KAFKA_SSL_CLIENT_AUTH-}" ]] && ( [[ $KAFKA_SSL_CLIENT_AUTH == *"required"* ]] || [[ $KAFKA_SSL_CLIENT_AUTH == *"requested"* ]] ) ; then  dub ensure KAFKA_SSL_TRUSTSTORE_FILENAME  && \
                      export KAFKA_SSL_TRUSTSTORE_LOCATION="/etc/kafka/secrets/$KAFKA_SSL_TRUSTSTORE_FILENAME" && \
                      dub path "$KAFKA_SSL_TRUSTSTORE_LOCATION" exists && \
                      dub ensure KAFKA_SSL_TRUSTSTORE_CREDENTIALS && \
                      KAFKA_SSL_TRUSTSTORE_CREDENTIALS_LOCATION="/etc/kafka/secrets/$KAFKA_SSL_TRUSTSTORE_CREDENTIALS" && \
                      dub path "$KAFKA_SSL_TRUSTSTORE_CREDENTIALS_LOCATION" exists && \
                      export KAFKA_SSL_TRUSTSTORE_PASSWORD && \
                      KAFKA_SSL_TRUSTSTORE_PASSWORD=$(cat "$KAFKA_SSL_TRUSTSTORE_CREDENTIALS_LOCATION") \
                  ; fi; fi && \
                # Set if KAFKA_ADVERTISED_LISTENERS has SASL_PLAINTEXT:// or SASL_SSL:// endpoints.  && \
                if [[ $KAFKA_ADVERTISED_LISTENERS =~ .*SASL_.*://.* ]]; then echo "SASL is enabled" && dub ensure KAFKA_OPTS && if [[ ! $KAFKA_OPTS == *"java.security.auth.login.config"*  ]]; then echo "KAFKA_OPTS should contain 'java.security.auth.login.config' property." ; fi ; fi  && \
                if [[ -n "${KAFKA_JMX_OPTS-}" ]]; then if [[ ! $KAFKA_JMX_OPTS == *"com.sun.management.jmxremote.rmi.port"*  ]] ; then echo "KAFKA_OPTS should contain 'com.sun.management.jmxremote.rmi.port' property. It is required for accessing the JMX metrics externally." ; fi ; fi  && \
                dub template "/etc/confluent/docker/${COMPONENT}.properties.template" "/etc/${COMPONENT}/${COMPONENT}.properties" && \
                dub template "/etc/confluent/docker/log4j.properties.template" "/etc/${COMPONENT}/log4j.properties" && \
                dub template "/etc/confluent/docker/tools-log4j.properties.template" "/etc/${COMPONENT}/tools-log4j.properties"  && \
                /etc/confluent/docker/configure  && \
                echo "Running preflight checks ... " && \
                /etc/confluent/docker/ensure  && \
                echo " Launching ... "  && \
                exec /etc/confluent/docker/launch  
# exec /etc/confluent/docker/run
              volumeMounts:
                - name: datadir-0
                  mountPath: /opt/kafka/data-0
        volumeClaimTemplates:
        - metadata:
            name: datadir-0
          spec:
            accessModes: [ "ReadWriteOnce" ]
            resources:
              requests:
                storage: "5Gi"